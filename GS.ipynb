{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "import concurrent.futures\n",
    "from typing import Dict, List\n",
    "import logging\n",
    "import urllib.request\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rearrange_and_dedup(doc_lists: List[List[Dict[str, str]]]) -> List[Dict[str, str]]:\n",
    "    doc_list = []\n",
    "    snippet_set = set()\n",
    "    # print([len(i) for i in doc_lists])\n",
    "    for i in range(50):\n",
    "        for ds in doc_lists:\n",
    "            if i < len(ds):\n",
    "                if 'snippet' in ds[i]:\n",
    "                    signature = ds[i]['snippet'].replace(' ', '')[:200]\n",
    "                else:\n",
    "                    signature = ds[i]['content'].replace(' ', '')[:200]\n",
    "                if signature not in snippet_set:\n",
    "                    doc_list.append(ds[i])\n",
    "                    snippet_set.add(signature)\n",
    "    return doc_list\n",
    "\n",
    "def search(query_list: List[str], n_max_doc: int = 20, search_engine: str = 'gdelt', freshness: str = '') -> List[Dict[str, str]]:\n",
    "    doc_lists = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = [executor.submit(search_single, query, search_engine, freshness) for query in query_list]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                doc_lists.append(future.result())\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    doc_list = _rearrange_and_dedup([d for d in doc_lists if d])\n",
    "    \n",
    "    return doc_list[:n_max_doc]\n",
    "\n",
    "def get_gdelt_data(query):\n",
    "    f = Filters(\n",
    "        keyword = query,\n",
    "        start_date = \"2024-09-27\",\n",
    "        end_date = \"2025-03-27\"\n",
    "    )\n",
    "\n",
    "    gd = GdeltDoc()\n",
    "\n",
    "    # Search for articles matching the filters\n",
    "    articles = gd.article_search(f)\n",
    "    #urls_to_query = articles['url'].to_list()\n",
    "    return articles\n",
    "\n",
    "\n",
    "def search_single(query: str, search_engine: str, freshness: str = '') -> List[Dict[str, str]]:\n",
    "    try:\n",
    "        if search_engine == 'gdelt':\n",
    "            #search_results = gdelt_request(query, freshness=freshness)\n",
    "            #return gdelt_format_results(search_results)\n",
    "            search_results = get_gdelt_data(query)\n",
    "            return gdelt_format_results(search_results)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f'Unsupported Search Engine: {search_engine}')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Search failed: {str(e)}')\n",
    "        raise ValueError(f'Search failed: {str(e)}')\n",
    "\n",
    "def get_snippet_from_url(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'Cache-Control': 'max-age=0'\n",
    "        }\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        with urllib.request.urlopen(req, timeout=5) as response:\n",
    "            html = response.read()\n",
    "            \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "            \n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching snippet from {url}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return content\n",
    "\n",
    "def gdelt_format_results(df):\n",
    "    search_results = df.to_dict('records')\n",
    "    formatted_results = [\n",
    "        {\n",
    "            'id': str(rank + 1),\n",
    "            'title': str(res.get('title', '')),\n",
    "            #'snippet': str(res.get('snippet', '')),\n",
    "            'snippet': get_snippet_from_url(res.get('url', '')) ,\n",
    "            'url': str(res.get('url', '')),\n",
    "            'timestamp': str(res.get('seendate', ''))[:11]\n",
    "        }\n",
    "        for rank, res in enumerate(search_results)\n",
    "    ]\n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "search([\"figure ai\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
